name: llamadart
description: A Dart/Flutter plugin for llama.cpp - run LLM inference on any platform using GGUF models
version: 0.4.1
homepage: https://github.com/leehack/llamadart
repository: https://github.com/leehack/llamadart
issue_tracker: https://github.com/leehack/llamadart/issues
topics:
  - llama
  - llm
  - ai
  - inference
  - gguf

environment:
  sdk: ^3.10.7
  flutter: '>=3.38.0'

dependencies:
  ffi: ^2.1.0
  path: ^1.8.3
  http: ^1.1.0
  web: ^1.0.0
  flutter:
    sdk: flutter
  code_assets: ^1.0.0
  hooks: ^1.0.0
  logging: ^1.3.0
  path_provider: ^2.1.0
  json_rpc_2: ^4.0.0
  dinja: ^1.0.0

dev_dependencies:
  ffigen: ^20.1.1
  lints: ^6.1.0
  test: ^1.26.3
  integration_test:
    sdk: flutter


# No plugin section needed for Pure Native Asset FFI package

