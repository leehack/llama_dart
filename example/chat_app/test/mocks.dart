import 'dart:async';
import 'package:llamadart/llamadart.dart';
import 'package:llamadart_chat_example/models/chat_settings.dart';
import 'package:llamadart_chat_example/services/chat_service.dart';
import 'package:llamadart_chat_example/services/settings_service.dart';

class MockLlamaBackend implements LlamaBackend {
  @override
  bool get isReady => true;
  @override
  Future<int> modelLoad(String path, ModelParams params) async => 1;
  @override
  Future<int> modelLoadFromUrl(
    String url,
    ModelParams params, {
    Function(double progress)? onProgress,
  }) async => 1;
  @override
  Future<void> modelFree(int modelHandle) async {}
  @override
  Future<int> contextCreate(int modelHandle, ModelParams params) async => 1;
  @override
  Future<void> contextFree(int contextHandle) async {}

  @override
  Future<int> getContextSize(int contextHandle) async => 2048;
  @override
  Stream<List<int>> generate(
    int contextHandle,
    String prompt,
    GenerationParams params, {
    List<LlamaContentPart>? parts,
  }) async* {
    yield [72, 105, 32, 116, 104, 101, 114, 101]; // "Hi there"
  }

  @override
  void cancelGeneration() {}
  @override
  Future<List<int>> tokenize(
    int modelHandle,
    String text, {
    bool addSpecial = true,
  }) async => [1, 2, 3];
  @override
  Future<String> detokenize(
    int modelHandle,
    List<int> tokens, {
    bool special = false,
  }) async => "mock";
  @override
  Future<Map<String, String>> modelMetadata(int modelHandle) async => {
    "llama.context_length": "2048",
  };
  @override
  Future<void> setLoraAdapter(
    int contextHandle,
    String path,
    double scale,
  ) async {}
  @override
  Future<void> removeLoraAdapter(int contextHandle, String path) async {}
  @override
  Future<void> clearLoraAdapters(int contextHandle) async {}
  @override
  Future<String> getBackendName() async => "Mock";
  @override
  bool get supportsUrlLoading => false;
  @override
  Future<bool> isGpuSupported() async => true;
  @override
  Future<void> setLogLevel(LlamaLogLevel level) async {}
  @override
  Future<void> dispose() async {}

  @override
  Future<int?> multimodalContextCreate(
    int modelHandle,
    String mmProjPath,
  ) async => 1;

  @override
  Future<void> multimodalContextFree(int mmContextHandle) async {}

  @override
  Future<bool> supportsAudio(int mmContextHandle) async => false;

  @override
  Future<bool> supportsVision(int mmContextHandle) async => false;

  @override
  Future<({int total, int free})> getVramInfo() async =>
      (total: 8 * 1024 * 1024 * 1024, free: 4 * 1024 * 1024 * 1024);

  @override
  Future<String> applyChatTemplate(
    int modelHandle,
    List<Map<String, dynamic>> messages, {
    String? customTemplate,
    bool addAssistant = true,
  }) async {
    return messages.map((m) => "${m['role']}: ${m['content']}").join('\n');
  }
}

class MockLlamaEngine extends LlamaEngine {
  bool initialized = false;

  MockLlamaEngine() : super(MockLlamaBackend());

  @override
  bool get isReady => initialized;

  @override
  Future<void> loadModel(
    String path, {
    ModelParams modelParams = const ModelParams(),
  }) async {
    initialized = true;
  }

  @override
  Future<void> loadModelFromUrl(
    String url, {
    ModelParams modelParams = const ModelParams(),
    Function(double progress)? onProgress,
  }) async {
    initialized = true;
  }

  @override
  Future<LlamaChatTemplateResult> chatTemplate(
    List<LlamaChatMessage> messages, {
    bool addAssistant = true,
    Map<String, dynamic>? jsonSchema,
    Object? tools,
  }) async {
    return const LlamaChatTemplateResult(
      prompt: "mock prompt",
      additionalStops: [],
      tokenCount: 5,
    );
  }

  @override
  Stream<LlamaCompletionChunk> create(
    List<LlamaChatMessage> messages, {
    GenerationParams? params,
    List<ToolDefinition>? tools,
    ToolChoice? toolChoice,
  }) async* {
    yield LlamaCompletionChunk(
      id: "mock-id",
      object: "chat.completion.chunk",
      created: 1234567890,
      model: "mock-model",
      choices: [
        LlamaCompletionChunkChoice(
          index: 0,
          delta: LlamaCompletionChunkDelta(content: "Hi there"),
        ),
      ],
    );
  }

  @override
  Future<int> getContextSize() async => 2048;

  @override
  Future<int> getTokenCount(String text) async => 5;
}

class MockSettingsService implements SettingsService {
  ChatSettings settings = const ChatSettings(modelPath: "mock.gguf");

  @override
  Future<ChatSettings> loadSettings() async => settings;

  @override
  Future<void> saveSettings(ChatSettings newSettings) async {
    settings = newSettings;
  }
}

class MockChatService extends ChatService {
  final MockLlamaEngine mockEngine;

  MockChatService({MockLlamaEngine? engine})
    : mockEngine = engine ?? MockLlamaEngine(),
      super(engine: engine ?? MockLlamaEngine());

  @override
  LlamaEngine get engine => mockEngine;

  @override
  Future<void> init(
    ChatSettings settings, {
    Function(double progress)? onProgress,
  }) async {
    if (settings.modelPath == null || settings.modelPath!.isEmpty) {
      throw Exception("Invalid model path");
    }
    await mockEngine.loadModel(settings.modelPath!);
  }

  @override
  String cleanResponse(String response) => response;
}
